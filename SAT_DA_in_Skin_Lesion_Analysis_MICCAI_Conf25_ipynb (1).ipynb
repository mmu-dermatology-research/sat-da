{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Selective Alignment Transfer for Domain Adaptation in Skin Lesion Analysis"
      ],
      "metadata": {
        "id": "wJJagDCq1Psl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "980gER0G1S1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((288, 288)),  ## EfficientNet-B2\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "l-fG4Zz-1Xh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Loading datasets\n",
        "train_dermoscopic = datasets.ImageFolder(\"/train/Derm\", transform=transform)\n",
        "train_clinical = datasets.ImageFolder(\"/train/Clinic\", transform=transform)\n",
        "val_dermoscopic = datasets.ImageFolder(\"/valid/Derm\", transform=transform)\n",
        "val_clinical = datasets.ImageFolder(\"/valid/Clinic\", transform=transform)\n",
        "\n",
        "\n",
        "## Creating dataloaders\n",
        "batch_size = 32\n",
        "\n",
        "train_dermos_loader = DataLoader(train_dermoscopic, batch_size=batch_size, shuffle=True)\n",
        "train_clinical_loader = DataLoader(train_clinical, batch_size=batch_size, shuffle=True)\n",
        "val_dermos_loader = DataLoader(val_dermoscopic, batch_size=batch_size, shuffle=False)\n",
        "val_clinical_loader = DataLoader(val_clinical, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "8ie5TObl1Xe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "\n",
        "\n",
        "## Feature Selector part\n",
        "\n",
        "class FeatureSelector(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(FeatureSelector, self).__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        ### Simple feed-forward network for feature selection\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(feature_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, feature_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, source_features, target_features):\n",
        "\n",
        "        if source_features.dim() == 1:\n",
        "            source_features = source_features.unsqueeze(0)\n",
        "        if target_features.dim() == 1:\n",
        "            target_features = target_features.unsqueeze(0)\n",
        "\n",
        "        ## Concatenatening domain features\n",
        "        combined = torch.cat([source_features, target_features], dim=1)\n",
        "\n",
        "        ## Generating feature importance weights\n",
        "        weights = self.network(combined)\n",
        "        return weights.squeeze(0)  # Removing batch dimension for output\n",
        "\n",
        "class DomainAdaptiveNet(nn.Module):\n",
        "    def __init__(self, base_model, feature_dim):\n",
        "        super(DomainAdaptiveNet, self).__init__()\n",
        "        self.features = base_model.features\n",
        "        self.feature_selector = FeatureSelector(feature_dim)\n",
        "        self.classifier = base_model.classifier\n",
        "        self.last_feature_weights = None\n",
        "\n",
        "    def forward(self, source_images, target_images=None):\n",
        "        # Extracting features\n",
        "        source_features = self.features(source_images)\n",
        "        B, C, H, W = source_features.shape\n",
        "        source_features_flat = source_features.view(B, C, -1).mean(dim=2)\n",
        "\n",
        "        if self.training and target_images is not None:\n",
        "            target_features = self.features(target_images)\n",
        "            target_features_flat = target_features.view(B, C, -1).mean(dim=2)\n",
        "\n",
        "            ### Getting mean features for each domain\n",
        "            source_mean = torch.mean(source_features_flat, dim=0)\n",
        "            target_mean = torch.mean(target_features_flat, dim=0)\n",
        "\n",
        "            ### Learning feature importance weights\n",
        "            feature_weights = self.feature_selector(source_mean, target_mean)\n",
        "\n",
        "            ### Applyig feature weights to both domains\n",
        "            source_features = source_features * feature_weights.view(1, -1, 1, 1)\n",
        "            target_features = target_features * feature_weights.view(1, -1, 1, 1)\n",
        "\n",
        "            ## Global average pooling\n",
        "            source_pooled = F.adaptive_avg_pool2d(source_features, (1, 1)).view(B, -1)\n",
        "            target_pooled = F.adaptive_avg_pool2d(target_features, (1, 1)).view(B, -1)\n",
        "\n",
        "            ## Classification task\n",
        "            source_output = self.classifier(source_pooled)\n",
        "            target_output = self.classifier(target_pooled)\n",
        "\n",
        "            ### Storing weights for inference\n",
        "            self.last_feature_weights = feature_weights.detach()\n",
        "\n",
        "            ### Storing intermediate features for alignment loss\n",
        "            self.source_features_flat = source_features_flat\n",
        "            self.target_features_flat = target_features_flat\n",
        "\n",
        "            return source_output, target_output, feature_weights\n",
        "\n",
        "        else:\n",
        "            ## Inference mode - only process source images\n",
        "            if self.last_feature_weights is None:\n",
        "                self.last_feature_weights = torch.ones(C, device=source_features.device)\n",
        "            source_features = source_features * self.last_feature_weights.view(1, -1, 1, 1)\n",
        "            source_pooled = F.adaptive_avg_pool2d(source_features, (1, 1)).view(B, -1)\n",
        "            return self.classifier(source_pooled)\n",
        "\n",
        "def train_model(model, train_clinical_loader, train_dermos_loader, val_clinical_loader, val_dermos_loader,\n",
        "                criterion, optimizer, num_epochs=50, save_path='./SAT-DA.pth'):\n",
        "    device = next(model.parameters()).device\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ## Training phase\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_source = 0\n",
        "        correct_target = 0\n",
        "        total = 0\n",
        "\n",
        "        for (source_images, source_labels), (target_images, target_labels) in zip(\n",
        "            train_clinical_loader, train_dermos_loader):\n",
        "\n",
        "            source_images, source_labels = source_images.to(device), source_labels.to(device)\n",
        "            target_images, target_labels = target_images.to(device), target_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            ## Forward pass\n",
        "            source_output, target_output, feature_weights = model(source_images, target_images)\n",
        "\n",
        "            ## Losses and backward pass (same as before)\n",
        "            source_loss = criterion(source_output, source_labels)\n",
        "            target_loss = criterion(target_output, target_labels)\n",
        "\n",
        "            ## Feature alignment loss\n",
        "            weighted_source_features = model.source_features_flat * feature_weights.unsqueeze(0)\n",
        "            weighted_target_features = model.target_features_flat * feature_weights.unsqueeze(0)\n",
        "\n",
        "            source_mean = weighted_source_features.mean(dim=0)\n",
        "            target_mean = weighted_target_features.mean(dim=0)\n",
        "\n",
        "            alignment_loss = F.mse_loss(source_mean, target_mean)\n",
        "            diversity_loss = -torch.std(feature_weights)\n",
        "\n",
        "            loss = source_loss + target_loss + 0.1 * alignment_loss + 0.01 * diversity_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            ### Calculating accuracies\n",
        "            _, predicted_source = torch.max(source_output.data, 1)\n",
        "            _, predicted_target = torch.max(target_output.data, 1)\n",
        "            total += source_labels.size(0)\n",
        "            correct_source += (predicted_source == source_labels).sum().item()\n",
        "            correct_target += (predicted_target == target_labels).sum().item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        ## Validation phase\n",
        "        model.eval()\n",
        "        val_correct_source = 0\n",
        "        val_correct_target = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            ## Validate source domain\n",
        "            for val_source_images, val_source_labels in val_clinical_loader:\n",
        "                val_source_images = val_source_images.to(device)\n",
        "                val_source_labels = val_source_labels.to(device)\n",
        "\n",
        "                val_source_output = model(val_source_images)\n",
        "                _, val_predicted_source = torch.max(val_source_output.data, 1)\n",
        "                val_correct_source += (val_predicted_source == val_source_labels).sum().item()\n",
        "                val_total += val_source_labels.size(0)\n",
        "\n",
        "            ## Validate target domain\n",
        "            for val_target_images, val_target_labels in val_dermos_loader:\n",
        "                val_target_images = val_target_images.to(device)\n",
        "                val_target_labels = val_target_labels.to(device)\n",
        "\n",
        "                val_target_output = model(val_target_images)\n",
        "                _, val_predicted_target = torch.max(val_target_output.data, 1)\n",
        "                val_correct_target += (val_predicted_target == val_target_labels).sum().item()\n",
        "\n",
        "        ### Calculating accuracies\n",
        "        train_source_acc = 100 * correct_source / total\n",
        "        train_target_acc = 100 * correct_target / total\n",
        "        val_source_acc = 100 * val_correct_source / val_total\n",
        "        val_target_acc = 100 * val_correct_target / val_total\n",
        "\n",
        "        ## Average validation accuracy\n",
        "        val_avg_acc = (val_source_acc + val_target_acc) / 2\n",
        "\n",
        "        ## Simple model saving\n",
        "        if val_avg_acc > best_val_acc:\n",
        "            best_val_acc = val_avg_acc\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Best model saved with validation accuracy: {val_avg_acc:.2f}%\")\n",
        "\n",
        "        ### Printing statistics\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Loss: {total_loss/len(train_clinical_loader):.4f}')\n",
        "        print(f'Train - Source Acc: {train_source_acc:.2f}%, Target Acc: {train_target_acc:.2f}%')\n",
        "        print(f'Val - Source Acc: {val_source_acc:.2f}%, Target Acc: {val_target_acc:.2f}%')\n",
        "\n",
        "        if hasattr(model, 'last_feature_weights'):\n",
        "            weights = model.last_feature_weights\n",
        "            print(f'Feature weights - Mean: {weights.mean().item():.4f}, '\n",
        "                  f'Std: {weights.std().item():.4f}, '\n",
        "                  f'Max: {weights.max().item():.4f}, '\n",
        "                  f'Min: {weights.min().item():.4f}')"
      ],
      "metadata": {
        "id": "1drslijX1Xb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize"
      ],
      "metadata": {
        "id": "kMnjTE6-2g8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Initialize model\n",
        "base_model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n",
        "feature_dim = 1408\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DomainAdaptiveNet(base_model, feature_dim).to(device)\n",
        "\n",
        "## Train model with validation\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_model(model, train_clinical_loader, train_dermos_loader,\n",
        "           val_clinical_loader, val_dermos_loader,\n",
        "           criterion, optimizer)"
      ],
      "metadata": {
        "id": "BtQbVilW1XYa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}